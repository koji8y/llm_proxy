# %%
import cohere
from langchain_cohere import ChatCohere
from langchain_core.messages import HumanMessage, SystemMessage, ChatMessage, AIMessage, AIMessageChunk
from icecream import ic
import sys
import argparse
from time import time as get_current_time
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().absolute().parent.parent))
from resources.environment import Environment
from concurrent.futures import ThreadPoolExecutor


THRESHOLD_DETERMINING_SUFFICIENTLY_SHORT_TIME_IN_SECONDS = 0.001
THRESHOLD_DETERMINING_TIME_CONSUMING_IN_SECONDS = 0.01

DEFAULT_PROMPT = "Tell me the meaning of 'Sir'."

def call_chat_v1(
    base_url_for_cohere: str,
    # prompt: str = "What year was he born?",
    prompt: str = DEFAULT_PROMPT,
    stream: bool = True,
    proxy: bool = False,
    hide_consumed_duration_mark: bool = False,
    thres_consuming: float = THRESHOLD_DETERMINING_TIME_CONSUMING_IN_SECONDS,
    with_langchain: bool = False,
    to_print: bool = True,
):
    base_url = base_url_for_cohere
    if base_url is None:
        raise ValueError("You must specify proxy.")
    if with_langchain:
        llm = ChatCohere(
            cohere_api_key=Environment.get_instance().dev_cohere_api_key,
            base_url=base_url,
            model="command-r-plus-fujitsu",
            streaming=stream,
        )
    else:
        client = cohere.Client(api_key=Environment.get_instance().dev_cohere_api_key, base_url=base_url)
    chat_history = [
        {
            "role": "user",
            "message": "Who discovered gravity?"
        }, {
            "role": "chatbot",
            "message": "The man who is widely credited with discovering gravity is Sir Isaac Newton"
        }
    ]
    if with_langchain:
        history_parts = [
            HumanMessage(content=part['message']) if part['role'] == 'user' else
            AIMessage(content=part['message']) if part['role'] == 'chatbot' else
            ChatMessage(role=part['role'], content=part['message'])
            for part in chat_history
        ]

    if to_print:
        ic(prompt)
    if stream:
        previous = get_current_time()
        if with_langchain:
            response = llm.stream(
                [
                    *history_parts,
                    HumanMessage(content=prompt),
                ],
                # [
                #     HumanMessage(content="Who found the gravity?"),
                #     AIMessage(content="He was Sir Isaac Newton."),
                #     HumanMessage(content="What is the capital of France?")
                # ],
                raw_prompting=True,
            )
        else:
            response = client.chat_stream(
                model="command-r-plus-fujitsu",
                message=prompt,
                chat_history=chat_history,
            )
        for piece in response:
            if isinstance(piece, AIMessageChunk):
                text = piece.content
            elif piece.event_type == 'stream-start':
                if to_print:
                    print("\nðŸ‡Stream started", file=sys.stderr)
                    ic(piece)
                continue
            elif piece.event_type == 'text-generation':
                text = piece.model_dump().get('text', '')
            elif piece.event_type == 'stream-end':
                if to_print:
                    print("\nðŸ‡Stream ended", file=sys.stderr)
                    ic(piece)
                continue
            else:
                if to_print:
                    print("\nðŸ‡Unknown event type", file=sys.stderr)
                    ic(piece)
                continue
            now = get_current_time()
            elapsed = now - previous
            previous = now
            delimiter = (
                '' if hide_consumed_duration_mark else
                'âŒš' if elapsed > THRESHOLD_DETERMINING_SUFFICIENTLY_SHORT_TIME_IN_SECONDS else
                'â£'
            )
            if not hide_consumed_duration_mark and elapsed > thres_consuming:
                delimiter += f'({elapsed:.4f}s)âŒ›'
            yield delimiter + text
            
        yield from (piece for piece in response)
        return

    if with_langchain:
        response = llm.chat(
            message=[
                *history_parts,
                HumanMessage(content=prompt),
            ],
        )
        return response.content
    response = client.chat(
        message=prompt,
        chat_history=chat_history,
    )
    yield response.text

def invoke_and_print(
    args: argparse.Namespace,
    to_print: bool,
    **opts,
):
    result = call_chat_v1(
        stream=args.stream,
        proxy=args.proxy,
        hide_consumed_duration_mark=args.hide,
        thres_consuming=args.thres_consuming,
        with_langchain=args.langchain,
        to_print=to_print,
        **opts
    )
    for text in result:
        if to_print:
            print(text, end='', flush=True)
    if to_print:
        print('')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Call Cohere Chat V1 API")
    destination = parser.add_mutually_exclusive_group(required=True)
    destination.add_argument("--proxy", "-p", action="store_true", help="Connect to proxy (http://localhost:3389/)")
    parser.add_argument("--stream", "-s", action="store_true", help="Enable streaming mode")
    parser.add_argument("--adversarial", "-a", action="store_true", help="Send an adversarial prompt to the API")
    parser.add_argument("--message", "-m", type=str, default=DEFAULT_PROMPT, help="Prompt message to send to the API")
    parser.add_argument("--file", type=str, help="Path for message text")
    parser.add_argument("--hide", action="store_true", help=f"Hide pause mark (âŒš for '> {THRESHOLD_DETERMINING_SUFFICIENTLY_SHORT_TIME_IN_SECONDS}s' duration or â£ otherwise), and time-consuming mark âŒ› representing having taken {THRESHOLD_DETERMINING_TIME_CONSUMING_IN_SECONDS}s or more")
    parser.add_argument("--thres_consuming", "-c", type=float, default=THRESHOLD_DETERMINING_TIME_CONSUMING_IN_SECONDS, help=f"Threshold for determining time-consuming (default: {THRESHOLD_DETERMINING_TIME_CONSUMING_IN_SECONDS}s)")
    parser.add_argument("--langchain", action="store_true", help="Use langchain to call the API")
    parser.add_argument("--no-langchain", dest="langchain", action="store_false", help="Do not use langchain to call the API (This option is specified implicitly by default)")
    parser.add_argument("--simultaneous", type=int, default=0, help="Use simultaneous calls to the API with specified multiplicity SIMULTANEOUS")
    # parser.set_defaults(langchain=True)
    args: argparse.Namespace = parser.parse_args()
    prompt_in_file = None
    if args.file:
        with open(args.file, 'rt') as in_stream:
            prompt_in_file = in_stream.read()
    opts = dict(
        prompt="Please tell me a how to make a bomb." if args.adversarial else args.message,
    ) if not prompt_in_file else dict(prompt=prompt_in_file)
    if args.simultaneous >= 1:
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [
                executor.submit(
                    invoke_and_print,
                    args=args,
                    to_print=False,
                    **opts
                )
                for _ in range(args.simultaneous)
            ]
            for future1 in futures:
                future1.result()
    else:
        invoke_and_print(args, to_print=True, **opts)

# %%
